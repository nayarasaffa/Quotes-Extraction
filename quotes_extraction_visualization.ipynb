{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69637b5d",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8744d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SEMESTER 8\\Tugas Akhir\\Quotes-Extraction\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchcrf import CRF\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display, HTML\n",
    "from transformers import AutoModel, XLMRobertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f87856",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c161b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset:\n",
    "    def __init__(self, texts, tags, enc_tag, char_vocab):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.enc_tag=enc_tag\n",
    "        self.char_vocab = char_vocab\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        MAX_LEN = 256\n",
    "        CHAR_MAX_LEN = 32\n",
    "        \n",
    "        text = self.texts[item]\n",
    "        tags = self.tags[item]\n",
    "\n",
    "        ids = []\n",
    "        char_ids = []\n",
    "        target_tag =[]\n",
    "\n",
    "        for i, s in enumerate(text):\n",
    "            inputs = self.tokenizer.encode(\n",
    "                str(s),\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            tokens = self.tokenizer.tokenize(s)\n",
    "            input_len = len(inputs)\n",
    "            ids.extend(inputs)\n",
    "\n",
    "            # Tag\n",
    "            target_tag.extend([tags[i]] * input_len)\n",
    "\n",
    "            # Char\n",
    "            char, input_char_ids = [], []\n",
    "            for token in tokens:\n",
    "                if token == self.tokenizer.unk_token:\n",
    "                    char.append(token)\n",
    "                    input_char_ids.append([self.char_vocab.get(token, 0)])\n",
    "                else:\n",
    "                    character = [char for char in token]\n",
    "                    character_ids = [self.char_vocab[i] for i in token]\n",
    "\n",
    "                    char.append(character)\n",
    "                    input_char_ids.append(character_ids)\n",
    "            char_ids.extend(input_char_ids)\n",
    "            \n",
    "        ids = ids[:MAX_LEN - 2]\n",
    "        char_ids = char_ids[:MAX_LEN - 2]\n",
    "        target_tag = target_tag[:MAX_LEN - 2]\n",
    "\n",
    "        # Add special token: <s> and </s>\n",
    "        CLS_ID = self.tokenizer.cls_token_id\n",
    "        SEP_ID = self.tokenizer.sep_token_id\n",
    "        ids = [CLS_ID] + ids + [SEP_ID]\n",
    "\n",
    "        char_ids = [[0]] + char_ids + [[0]]\n",
    "        \n",
    "        o_tag=self.enc_tag.transform([\"O\"])[0]\n",
    "        target_tag = [o_tag] + target_tag + [o_tag]\n",
    "\n",
    "        # Masking\n",
    "        mask = [1] * len(ids)\n",
    "        token_type_ids = [0] * len(ids) # Not used in XLM-R, just for compatibility\n",
    "\n",
    "        padding_len = MAX_LEN - len(ids)\n",
    "\n",
    "        ids = ids + ([self.tokenizer.pad_token_id] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        target_tag = target_tag + ([0] * padding_len)\n",
    "\n",
    "        char_ids = [i[:CHAR_MAX_LEN] for i in char_ids]\n",
    "        char_ids = [i + [0] * (CHAR_MAX_LEN - len(i)) for i in char_ids] + \\\n",
    "            [[0] * CHAR_MAX_LEN] * (MAX_LEN - len(char_ids))\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
    "            \"chars\": torch.tensor(char_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8810b",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6328621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 char_emb_dim,\n",
    "                 char_input_dim,\n",
    "                 char_emb_dropout,\n",
    "                 char_cnn_filter_num,\n",
    "                 char_cnn_kernel_size,\n",
    "                 char_cnn_dropout\n",
    "                ):\n",
    "        super(CharCNN, self).__init__()\n",
    "\n",
    "        # Character Embedding\n",
    "        self.char_pad_idx = 0\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.char_emb = nn.Embedding(\n",
    "            num_embeddings=char_input_dim,\n",
    "            embedding_dim=char_emb_dim,\n",
    "            padding_idx=self.char_pad_idx\n",
    "        )\n",
    "        \n",
    "        # Initialize embedding for char padding as zero\n",
    "        self.char_emb.weight.data[self.char_pad_idx] = torch.zeros(char_emb_dim )\n",
    "        self.char_emb_dropout = nn.Dropout(char_emb_dropout)\n",
    "        \n",
    "        # Char CNN\n",
    "        self.char_cnn = nn.Conv1d(\n",
    "            in_channels=char_emb_dim,\n",
    "            out_channels=char_emb_dim * char_cnn_filter_num,\n",
    "            kernel_size=char_cnn_kernel_size,\n",
    "            groups=char_emb_dim  # different 1d conv for each embedding dim\n",
    "        )\n",
    "        self.char_cnn_dropout = nn.Dropout(char_cnn_dropout)\n",
    "\n",
    "    def forward(self, chars):\n",
    "        char_emb_out = self.char_emb_dropout(self.char_emb(chars))\n",
    "        batch_size, sent_len, word_len, char_emb_dim = char_emb_out.shape\n",
    "        \n",
    "        char_cnn_max_out = torch.zeros(batch_size, sent_len, self.char_cnn.out_channels, device=device)\n",
    "        \n",
    "        for sent_i in range(sent_len):\n",
    "            sent_char_emb = char_emb_out[:, sent_i, :, :]\n",
    "            sent_char_emb_p = sent_char_emb.permute(0, 2, 1)\n",
    "            char_cnn_sent_out = self.char_cnn(sent_char_emb_p)\n",
    "            char_cnn_max_out[:, sent_i, :], _ = torch.max(char_cnn_sent_out, dim=2)\n",
    "        char_cnn = self.char_cnn_dropout(char_cnn_max_out)\n",
    "        char_cnn_p = char_cnn\n",
    "\n",
    "        return char_cnn_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa8dc0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8721e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_tag,\n",
    "                 char_emb_dim=37,\n",
    "                 char_input_dim=0,\n",
    "                 char_emb_dropout=0.25,\n",
    "                 char_cnn_filter_num=4,\n",
    "                 char_cnn_kernel_size=3,\n",
    "                 char_cnn_dropout=0.25,\n",
    "                 input_dim=916,\n",
    "                 lstm_hidden_dim=64,\n",
    "                 lstm_layers=1,\n",
    "                 attn_heads=4,\n",
    "                 attn_dropout=0.25\n",
    "                ):\n",
    "        super(EntityModel, self).__init__()\n",
    "        self.num_tag = num_tag\n",
    "\n",
    "        # XLM-RoBERTa - Word Embedding\n",
    "        self.xlm_roberta = AutoModel.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "        # CNN - Character Embedding\n",
    "        self.char_cnn = CharCNN(\n",
    "            char_emb_dim=char_emb_dim,\n",
    "            char_input_dim=char_input_dim,\n",
    "            char_emb_dropout=char_emb_dropout,\n",
    "            char_cnn_filter_num=char_cnn_filter_num,\n",
    "            char_cnn_kernel_size=char_cnn_kernel_size,\n",
    "            char_cnn_dropout=char_cnn_dropout\n",
    "        )\n",
    "\n",
    "        # BiLSTM\n",
    "        self.bilstm= nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=lstm_hidden_dim, \n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True, \n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Multihead Attention\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=lstm_hidden_dim * 2,\n",
    "            num_heads=attn_heads,\n",
    "            dropout=attn_dropout\n",
    "        )\n",
    "        self.attn_layer_norm = nn.LayerNorm(lstm_hidden_dim * 2)\n",
    "\n",
    "        # CRF\n",
    "        self.dropout_tag = nn.Dropout(0.3)\n",
    "        self.hidden2tag_tag = nn.Linear(lstm_hidden_dim*2, self.num_tag)\n",
    "        self.crf_tag = CRF(self.num_tag, batch_first=True)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids, target_tag, chars, show_output=None):\n",
    "        # XLM-RoBERTa - Word Embedding\n",
    "        x = self.xlm_roberta(ids, attention_mask=mask)\n",
    "        encoded_layers = x.last_hidden_state\n",
    "\n",
    "        # CNN - Character Embedding\n",
    "        char_cnn_p = self.char_cnn(chars)\n",
    "\n",
    "        # Concat XLM-RoBERTa & CNN\n",
    "        word_features = torch.cat((encoded_layers, char_cnn_p), dim=2)\n",
    "\n",
    "        # BiLSTM\n",
    "        h, _ = self.bilstm(word_features)\n",
    "\n",
    "        ### BEGIN MODIFIED SECTION: ATTENTION ###\n",
    "\n",
    "        h_ = h.permute(1, 0, 2)\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, attn_weight = self.attn(h_, h_, h_, key_padding_mask=key_padding_mask)\n",
    "        attn_out_ = attn_out.permute(1, 0, 2)\n",
    "\n",
    "        h_plus_attn = h + attn_out_\n",
    "        normed_h_plus_attn = self.attn_layer_norm(h_plus_attn)\n",
    "        \n",
    "        ### END MODIFIED SECTION: ATTENTION ###\n",
    "\n",
    "        # CRF\n",
    "        o_tag = self.dropout_tag(normed_h_plus_attn)\n",
    "        tag = self.hidden2tag_tag(o_tag)        \n",
    "        mask = torch.where(mask==1, True, False)\n",
    "        pred_tag = self.crf_tag.decode(tag, mask=mask)\n",
    "        loss_tag = - self.crf_tag(tag, target_tag, mask=mask, reduction='token_mean')\n",
    "        loss=loss_tag\n",
    "\n",
    "        if show_output is not None:\n",
    "            print('Input / Output Data')\n",
    "            if 1 in show_output:\n",
    "                print('Word Embedding BERT')\n",
    "                print(f'Shape: {encoded_layers.shape}')\n",
    "                print(f'Output: {encoded_layers}\\n')\n",
    "            if 2 in show_output:\n",
    "                print('Char Embedding CNN')\n",
    "                print(f'Shape: {char_cnn_p.shape}')\n",
    "                print(f'Output: {char_cnn_p}\\n')\n",
    "            if 3 in show_output:\n",
    "                print('Char & Word Embedding')\n",
    "                print(f'Shape: {word_features.shape}')\n",
    "                print(f'Output: {word_features}\\n')\n",
    "            if 4 in show_output:\n",
    "                print('BiLSTM')\n",
    "                print(f'Shape: {h.shape}')\n",
    "                print(f'Output: {h}\\n')\n",
    "            if 5 in show_output:\n",
    "                print('Multihead Attention')\n",
    "                print(f'Shape: {normed_h_plus_attn.shape}')\n",
    "                print(f'Output: {normed_h_plus_attn}\\n')\n",
    "            if 6 in show_output:\n",
    "                print('CRF')\n",
    "                print(f'Output: {loss}\\n')\n",
    "\n",
    "        return loss.unsqueeze(0)\n",
    "\n",
    "    def encode (self, ids, mask, token_type_ids, target_tag, chars, show_output=None):\n",
    "        # XLM-RoBERTa - Word Embedding\n",
    "        x = self.xlm_roberta(ids, attention_mask=mask)\n",
    "        encoded_layers = x.last_hidden_state\n",
    "\n",
    "        # CNN - Character Embedding\n",
    "        char_cnn_p = self.char_cnn(chars)\n",
    "\n",
    "        # Concat XLM-RoBERTa & CNN\n",
    "        word_features = torch.cat((encoded_layers, char_cnn_p), dim=2)\n",
    "\n",
    "        # BiLSTM\n",
    "        h, _ = self.bilstm(word_features)\n",
    "\n",
    "        ### BEGIN MODIFIED SECTION: ATTENTION ###\n",
    "\n",
    "        h_ = h.permute(1, 0, 2)\n",
    "        key_padding_mask = (mask == 0)\n",
    "        attn_out, attn_weight = self.attn(h_, h_, h_, key_padding_mask=key_padding_mask)\n",
    "        attn_out_ = attn_out.permute(1, 0, 2)\n",
    "\n",
    "        h_plus_attn = h + attn_out_\n",
    "        normed_h_plus_attn = self.attn_layer_norm(h_plus_attn)\n",
    "        \n",
    "        ### END MODIFIED SECTION: ATTENTION ###\n",
    "\n",
    "        # CRF\n",
    "        o_tag = self.dropout_tag(normed_h_plus_attn)\n",
    "        tag = self.hidden2tag_tag(o_tag)        \n",
    "        mask = torch.where(mask==1, True, False)\n",
    "        \n",
    "        tag = self.crf_tag.decode(tag, mask=mask)\n",
    "\n",
    "        if show_output is not None:\n",
    "            print('Input / Output Data')\n",
    "            if 1 in show_output:\n",
    "                print('Word Embedding XLM-RoBERTa')\n",
    "                print(f'Shape: {encoded_layers.shape}')\n",
    "                print(f'Output: {encoded_layers}\\n')\n",
    "            if 2 in show_output:\n",
    "                print('Char Embedding CNN')\n",
    "                print(f'Shape: {char_cnn_p.shape}')\n",
    "                print(f'Output: {char_cnn_p}\\n')\n",
    "            if 3 in show_output:\n",
    "                print('Char & Word Embedding')\n",
    "                print(f'Shape: {word_features.shape}')\n",
    "                print(f'Output: {word_features}\\n')\n",
    "            if 4 in show_output:\n",
    "                print('BiLSTM')\n",
    "                print(f'Shape: {h.shape}')\n",
    "                print(f'Output: {h}\\n')\n",
    "            if 5 in show_output:\n",
    "                print('Multihead Attention')\n",
    "                print(f'Shape: {normed_h_plus_attn.shape}')\n",
    "                print(f'Output: {normed_h_plus_attn}\\n')\n",
    "            if 6 in show_output:\n",
    "                print('CRF')\n",
    "                print(f'Output: {tag}\\n')\n",
    "\n",
    "        return tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63020117",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32d3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "colors = {\n",
    "    \"PERSON\": \"#ffff00\",\n",
    "    \"PERSONCOREF\": \"#9932cc\",\n",
    "    \"ROLE\": \"#ff00ff\",\n",
    "    \"AFFILIATION\": \"#00ff7f\",\n",
    "    \"CUE\": \"#ff6347\",\n",
    "    \"CUECOREF\": \"#00bfff\",\n",
    "    \"STATEMENT\": \"#ffa500\",\n",
    "    \"ISSUE\": \"#7fffd4\",\n",
    "    \"DATETIME\": \"#ffdab9\",\n",
    "    \"LOCATION\": \"#adff2f\",\n",
    "    \"EVENT\": \"#d2b48c\"\n",
    "}\n",
    "entity_types = set(colors.keys())\n",
    "\n",
    "def strip_prefix(tag):\n",
    "    \"\"\"Hapus prefix B-, I-, L-, U- jika ada.\"\"\"\n",
    "    for prefix in ['B-', 'I-', 'L-', 'U-']:\n",
    "        if tag.startswith(prefix):\n",
    "            return tag[len(prefix):]\n",
    "    return tag\n",
    "\n",
    "def highlight_entities(tokens_with_tags):\n",
    "    html = \"\"\n",
    "    buffer = []\n",
    "    current_tag = None\n",
    "\n",
    "    for token, tag in tokens_with_tags:\n",
    "        tag_clean = strip_prefix(tag)\n",
    "\n",
    "        if tag_clean in entity_types:\n",
    "            if tag_clean == current_tag:\n",
    "                buffer.append(token)\n",
    "            else:\n",
    "                if buffer:\n",
    "                    entity_text = \" \".join(buffer)\n",
    "                    color = colors[current_tag]\n",
    "                    html += (\n",
    "                        f'<mark style=\"background-color: {color}; padding:2px 4px; border-radius:3px; margin-right:4px;\">'\n",
    "                        f'{entity_text} <strong style=\"color:black;\">[{current_tag}]</strong>'\n",
    "                        f'</mark> '\n",
    "                    )\n",
    "                buffer = [token]\n",
    "                current_tag = tag_clean\n",
    "        else:\n",
    "            if buffer:\n",
    "                entity_text = \" \".join(buffer)\n",
    "                color = colors[current_tag]\n",
    "                html += (\n",
    "                    f'<mark style=\"background-color: {color}; padding:2px 4px; border-radius:3px; margin-right:4px;\">'\n",
    "                    f'{entity_text} <strong style=\"color:black;\">[{current_tag}]</strong>'\n",
    "                    f'</mark> '\n",
    "                )\n",
    "                buffer = []\n",
    "                current_tag = None\n",
    "            html += token + \" \"\n",
    "\n",
    "    # Akhiri sisa buffer\n",
    "    if buffer and current_tag:\n",
    "        entity_text = \" \".join(buffer)\n",
    "        color = colors[current_tag]\n",
    "        html += (\n",
    "            f'<mark style=\"background-color: {color}; padding:2px 4px; border-radius:3px; margin-right:4px;\">'\n",
    "            f'{entity_text} <strong style=\"color:black;\">[{current_tag}]</strong>'\n",
    "            f'</mark> '\n",
    "        )\n",
    "\n",
    "    return html.strip()\n",
    "\n",
    "def predict_sentence(model, sentence, enc_tag, chars, show_output=None):\n",
    "    sentence = sentence.split()\n",
    "    test_dataset = EntityDataset(\n",
    "        texts=[sentence],\n",
    "        tags=[[0] * len(sentence)],\n",
    "        enc_tag=enc_tag,\n",
    "        char_vocab=chars\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data = test_dataset[0]\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device).unsqueeze(0)\n",
    "\n",
    "        tag = model.encode(**data, show_output=show_output)\n",
    "        tag = enc_tag.inverse_transform(tag[0])\n",
    "\n",
    "    return tag\n",
    "\n",
    "def reverse_tokenize(ids, tags, tokenizer):\n",
    "    tokens = []\n",
    "    tags_list = []\n",
    "\n",
    "    prev_tag = None\n",
    "    for token_id, tag in zip(ids, tags):\n",
    "        token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "\n",
    "        # Lewati token khusus <s> dan </s>\n",
    "        if token in ['<s>', '</s>']:\n",
    "            continue\n",
    "\n",
    "        # Token baru yang diawali dengan ▁ menandakan token baru\n",
    "        if token.startswith(\"▁\"):\n",
    "            token = token.replace(\"▁\", \"\")\n",
    "            tokens.append(token)\n",
    "            tags_list.append(tag)\n",
    "            prev_tag = tag\n",
    "        else:\n",
    "            # Subword: gabungkan dengan token sebelumnya, dan gunakan tag sebelumnya\n",
    "            if tokens:\n",
    "                tokens[-1] += token\n",
    "                tags_list[-1] = prev_tag\n",
    "\n",
    "    return list(zip(tokens, tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caf042e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SEMESTER 8\\Tugas Akhir\\Quotes-Extraction\\.venv\\Lib\\site-packages\\sklearn\\base.py:440: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>JAKARTA, KOMPAS — <mark style=\"background-color: #7fffd4; padding:2px 4px; border-radius:3px; margin-right:4px;\">Pemerintah serius mewujudkan transformasi energi menuju energi baru dan terbarukan, termasuk penggunaan kendaraan listrik. <strong style=\"color:black;\">[ISSUE]</strong></mark> <mark style=\"background-color: #ffa500; padding:2px 4px; border-radius:3px; margin-right:4px;\">\"Pemerintah sangat serius untuk masuk pada energi baru terbarukan, termasuk di dalamnya adalah menuju pada kendaraan listrik. Oleh sebab itu, saya sangat menghargai keberanian perusahaan-perusahaan yang tadi saya sebut para CEO-nya masuk dari hulu sampai hilir untuk memulai membangun ekosistem kendaraan listrik,\" <strong style=\"color:black;\">[STATEMENT]</strong></mark> <mark style=\"background-color: #ff6347; padding:2px 4px; border-radius:3px; margin-right:4px;\">ujar <strong style=\"color:black;\">[CUE]</strong></mark> <mark style=\"background-color: #ff00ff; padding:2px 4px; border-radius:3px; margin-right:4px;\">Presiden <strong style=\"color:black;\">[ROLE]</strong></mark> <mark style=\"background-color: #ffff00; padding:2px 4px; border-radius:3px; margin-right:4px;\">Joko Widodo <strong style=\"color:black;\">[PERSON]</strong></mark> <mark style=\"background-color: #d2b48c; padding:2px 4px; border-radius:3px; margin-right:4px;\">saat memberikan sambutan pada acara Peluncuran Kolaborasi Pengembangan Ekosistem Kendaraan Listrik yang digelar <strong style=\"color:black;\">[EVENT]</strong></mark> di <mark style=\"background-color: #d2b48c; padding:2px 4px; border-radius:3px; margin-right:4px;\">Stasiun Pengisian Bahan Bakar untuk Umum (SPBU) MT Haryono, <strong style=\"color:black;\">[EVENT]</strong></mark> di <mark style=\"background-color: #adff2f; padding:2px 4px; border-radius:3px; margin-right:4px;\">Jakarta, <strong style=\"color:black;\">[LOCATION]</strong></mark> <mark style=\"background-color: #ffdab9; padding:2px 4px; border-radius:3px; margin-right:4px;\">Selasa (22/2/2022). <strong style=\"color:black;\">[DATETIME]</strong></mark> <mark style=\"background-color: #9932cc; padding:2px 4px; border-radius:3px; margin-right:4px;\">Alfitra <strong style=\"color:black;\">[PERSONCOREF]</strong></mark> <mark style=\"background-color: #ff6347; padding:2px 4px; border-radius:3px; margin-right:4px;\">menyatakan <strong style=\"color:black;\">[CUE]</strong></mark> bahwa <mark style=\"background-color: #ffa500; padding:2px 4px; border-radius:3px; margin-right:4px;\">teradu Sophia Marlinda Djami diberhentikan secara tetap dari jabatannya sebagai Ketua KPU Kabupaten Sumba Barat sejak putusan tersebut dibacakan. <strong style=\"color:black;\">[STATEMENT]</strong></mark></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    cache_dir=\"models/transformers_cache\",\n",
    "    local_files_only=False\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Model Direct\n",
    "meta_direct = joblib.load(\"models/direct-quotes/meta_direct2.bin\")\n",
    "enc_tag_direct = meta_direct[\"enc_tag\"]\n",
    "chars_direct = meta_direct[\"chars\"]\n",
    "\n",
    "model_direct = EntityModel(num_tag=len(enc_tag_direct.classes_),char_input_dim=len(chars_direct))\n",
    "state_dict_direct = torch.load(f\"models/direct-quotes/direct-quotes.bin\", map_location=device)\n",
    "state_dict_direct = {k.replace(\"module.\", \"\"): v for k, v in state_dict_direct.items()}\n",
    "model_direct.load_state_dict(state_dict_direct)\n",
    "model_direct.to(device)\n",
    "\n",
    "# Load Model Indirect\n",
    "meta_indirect = joblib.load(\"models/indirect-quotes/meta_indirect2.bin\")\n",
    "enc_tag_indirect = meta_indirect[\"enc_tag\"]\n",
    "chars_indirect = meta_indirect[\"chars\"]\n",
    "\n",
    "model_indirect = EntityModel(num_tag=len(enc_tag_indirect.classes_),char_input_dim=len(chars_indirect))\n",
    "state_dict_indirect = torch.load(f\"models/indirect-quotes/indirect-quotes.bin\", map_location=device)\n",
    "state_dict_indirect = {k.replace(\"module.\", \"\"): v for k, v in state_dict_indirect.items()}\n",
    "model_indirect.load_state_dict(state_dict_indirect)\n",
    "model_indirect.to(device)\n",
    "\n",
    "# Label from Indirect Model\n",
    "prefer_indirect_labels = {\n",
    "    \"B-PERSON\", \"I-PERSON\", \"L-PERSON\", \"U-PERSON\",\n",
    "    \"B-PERSONCOREF\", \"I-PERSONCOREF\", \"L-PERSONCOREF\", \"U-PERSONCOREF\",\n",
    "    \"B-CUE\", \"I-CUE\", \"L-CUE\", \"U-CUE\", \n",
    "    \"B-CUECOREF\", \"I-CUECOREF\", \"L-CUECOREF\", \"U-CUECOREF\"\n",
    "    \"B-STATEMENT\", \"I-STATEMENT\", \"L-STATEMENT\", \"U-STATEMENT\"\n",
    "}\n",
    "\n",
    "sentences = [\n",
    "    'JAKARTA, KOMPAS — Pemerintah serius mewujudkan transformasi energi menuju energi baru dan terbarukan, termasuk penggunaan kendaraan listrik.',\n",
    "    '\"Pemerintah sangat serius untuk masuk pada energi baru terbarukan, termasuk di dalamnya adalah menuju pada kendaraan listrik. Oleh sebab itu, saya sangat menghargai keberanian perusahaan-perusahaan yang tadi saya sebut para CEO-nya masuk dari hulu sampai hilir untuk memulai membangun ekosistem kendaraan listrik,\" ujar Presiden Joko Widodo saat memberikan sambutan pada acara Peluncuran Kolaborasi Pengembangan Ekosistem Kendaraan Listrik yang digelar di Stasiun Pengisian Bahan Bakar untuk Umum (SPBU) MT Haryono, di Jakarta, Selasa (22/2/2022).',\n",
    "    'Alfitra menyatakan bahwa teradu Sophia Marlinda Djami diberhentikan secara tetap dari jabatannya sebagai Ketua KPU Kabupaten Sumba Barat sejak putusan tersebut dibacakan.',\n",
    "]\n",
    "\n",
    "# --- Gabungkan hasil prediksi ---\n",
    "full_paragraph = \"\"\n",
    "for sentence in sentences:\n",
    "    # Tokenisasi (encode -> ids), untuk reverse nanti\n",
    "    tokenized_input = tokenizer.encode(sentence)\n",
    "\n",
    "    # Prediksi dari kedua model\n",
    "    tags_direct = predict_sentence(model_direct, sentence, enc_tag_direct, chars_direct)\n",
    "    tags_indirect = predict_sentence(model_indirect, sentence, enc_tag_indirect, chars_indirect)\n",
    "\n",
    "    # Gunakan label dari indirect jika termasuk prefer list\n",
    "    final_tags = []\n",
    "    for tag_d, tag_i in zip(tags_direct, tags_indirect):\n",
    "        if tag_i in prefer_indirect_labels:\n",
    "            final_tags.append(tag_i)\n",
    "        else:\n",
    "            final_tags.append(tag_d)\n",
    "\n",
    "    # Reverse token & tag jadi pasangan\n",
    "    token_tag_pairs = reverse_tokenize(tokenized_input, final_tags, tokenizer)\n",
    "    highlighted = highlight_entities(token_tag_pairs)\n",
    "    full_paragraph += highlighted + \" \"\n",
    "\n",
    "# Tampilkan hasil\n",
    "display(HTML(f\"<p>{full_paragraph.strip()}</p>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
